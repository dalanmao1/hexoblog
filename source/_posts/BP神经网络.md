---
title: BP神经网络
---


> ## 1. 人工神经网络

### 1.1 原理

人工神经网络(Artificial Neural Networks，ANN)也简称为神经网络(NN)，它是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。这种网络依靠系统的复杂程度，通过调整内部大量结点之间相互连接的关系，从而达到处理信息的目的。

神经网络首先要以一定的学习准则进行学习，以

决定神经网络模型性能的三大要素为： 神经元(信息处理单元)的特性； 神经元之间相互连接的形式——拓扑结构； 为适应环境而改善性能的学习规则。

### 1.2 特点

人工神经网络是有**自适应**能力的，它可以在训练或者学习过程中改变自身的权重，来适应环境要求。

- **训练方式**
  - 有监督：利用给定的样本进行分类或者模仿。
  - 无监督：规定学习规则，学习的内容随系统所处环境而异，系统可以自发的发现环境的特征，类似人脑的的功能。
- **泛化能力** 对那些没有训练过的样本，有很好的预测能力和控制能力，特别是存在一些噪声的样本。**比如**，公式$y=2x^2$的x和y的值作为样本，假设样本只取x=[0,100],y=x;神经网络训练完之后得到近似公式$y=2x^2$的数学模型,即使没有训练过x=999的这个样本，但是仍可以神经网络近似的数学模型知道y的值是多少。
- **模拟/辨识能力：** 当设计者很清楚的知道系统时，可以用数值分析，微积分等数学工具建立，但是当系统很复杂的时候，或者系统未知，信息量很少，建立精确的数学模型就很困难。神经网络的非线性映射能力就表现出优势，因为它不需要对系统有很透彻的了解，**只需要知道系统输入与输出的映射关系即可近似得到一个数学模型**。

> ## 2. BP神经网络

BP神经网络是一种典型的非线性算法。BP神经网络由**输入层、输出层和之间若干层（一层或多层）隐含层**构成，每一层可以有若干个节点。层与层之间节点的连接状态通过权重来体现。

![](https://img-blog.csdnimg.cn/1c40769c8cc94820a1d15aadf765a0a3.png#pic_center=60x60)

### 2.1 感知器

上图可以看到BP神经网络时由一个个节点和线连接起来组成的网络，组成这个网络的单元就叫**感知器**。

![](https://img-blog.csdnimg.cn/f2233f14be9542bdac84b8076e71f00b.png#pic_center=60x60)

感知器的结构由输入项$x_i$、权重$w_i$、偏置$\theta$、激活函数$f(\cdot)$、输出$y$组成。其中偏置和激活函数可能不太好理解为什么要把它们放在里面。

首先，偏置（有些文献叫阈值），用一张图来解释为什么要用它，神经网络的作用是对样本数据进行学习然后得到一个近似的模型，细想本质就是寻找规律然后对数据进行分类。

![](https://img-blog.csdnimg.cn/6bbff835924b401db44b0f6707f922a2.png)


为什么采用激活函数？
第一点，假设每个感知器的表达为$y=ax+b$，如果没有激活函数，当x的值也就是输入很大时，经过层层的计算，那这个数值将会非常非常大，激活函数的作用就是将感知器的输出限制在一个固定区间，比如[0,1]。
第二点，按照感知器的输出来看，它是线性的，就算再经过下一层的网络，它仍然是线性的，但是神经网络是需要有非线性的能力的，激活函数正好能赋予它这个能力。


sigmoid激活函数的特点：

- 上下有界（relu是个特例）
- 连续光滑，连续可微（不满足这一条后续就无法做偏导）
- 该函数的中区高增益部分解决了小信号需要放大的问题，两侧的低增益区适合处理大信号。

下图是sigmoid激活函数曲线，类似的有很多，就不一一列举了。
$$
\delta(x)=\frac{1}{1+e^{-x}}
$$

![](https://img-blog.csdnimg.cn/35d07c8636834b89abd0b5bfd5c59b29.png)

> ## 3. 正向传播和反向传播

BP神经网络的学习过程如下：

![](https://img-blog.csdnimg.cn/151c619cf6c1496584eded648678b29e.png)

### 3.1 正向传播

输入样本从输入层传入，经过各隐层处理之后，传向输出层。

![](https://img-blog.csdnimg.cn/7c30c24f60224ad9be18ccea46985323.png#pic_center=60x60)

输入层有n个神经元组成，
输入层的输出：$x_i(i=1,2,...,n)$

隐藏层有q个神经元组成,输入层和隐藏层的权值为$v_{ki}(i=1,2,...,n;k=1,2,...,q)$
隐藏层输入（含阈值$\theta$）：
$$
S_k=\sum_{i=0}^{n}v_{ki}\cdot x_i
$$
隐藏层输出：
$$
z_k=f(S_k)
$$

输出层由m个神经元组成，$y_j(i=j,2,...,m)$为该层的输出
隐藏层和输出层的权值为$w_{jk}(k=j,2,...,q;j=1,2,...,m)$
输出层输入：
$$
S_j=\sum_{k=0}^{q}w_{jk}\cdot z_k
$$
输出层输出：
$$
y_j=f(S_j)
$$

### 3.2 反向传播

正向传播是输入样本从输入层传入，经过各隐层处理之后，传向输出层。若输出层的实际输出与期望的输出不符合，则开始**反向传播**：将误差用某种形式通过隐藏层逐层反传，并将误差分摊给各层的神经元，从而获得各层神经元的误差信号，用于修正各个神经元的权值。
神经元的正向传播和反向传播是周而复始进行的，一直进行到误差减少到可以接受的程度为止，神经网络学习的过程就是不断修正权值的过程。

#### 3.2.1 梯度下降

当一个人站在山顶或者半山腰，想要以最快的速度到达山脚，那么哪条路最快呢？
是每一步都下降最快的路，变化最快。在数学上，用梯度的概念来描述。

下面用梯度下降法求函数的最小值问题：
![](https://img-blog.csdnimg.cn/02340bf852c3483498ee3a55ceceda3e.png)

![](https://img-blog.csdnimg.cn/58e158819a4f4b8ab25356c2af810b2d.png)

梯度是一个矢量，表示某一函数在该点处的方向导数沿着该方向取得最大值，只需要不断的求偏导进行迭代就可以以最快速度到达最小值。
这边的学习率也就是步长，如果取太长，会在最小值左右来回震荡；如果的取的太小，会增加学习时长，一般在[0,1]之间。

#### 3.2.2 公式推导

误差的形式一般采用：
$$
y_j=f(S_j)
$$


> ## 参考文献

[1]冷雨泉等. 《机器学习入门到实战MATLAB实践应用》[J]. 清华大学出版社, 2019