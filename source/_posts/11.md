---
title: 强化学习方法——时序差分学习
---
在强化学习所有的思想中，时序差分(TD)学习是最核心，最新颖的思想。蒙特卡洛(MC)往往需要等到完整的一轮结束了之后才能确定真实回报G(t)，但实际中很多场景完整一轮的时间非常长，或者根本没有结束状态，这时候用蒙特卡洛的方法就不适合了。

>## 时序差分学习(TD)

时序差分学习是一种从**经验片段**中进行学习的一种方法，与MC最大的从差距就是，TD方法不需要等到一轮结束（即等到终止状态），只需要等到下一个时刻即可。在t+1时刻，根据得到的奖励$R_{t+1}$和估计值$V(S_{t+1})$对当前的估计值$V(S_t)$进行跟新，定义为：
$$
V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$
括号里面的是误差，它是衡量$S_{t+1}$的估计值和更好的估计值$R_{t+1}+\gamma V(S_{t+1}$之间的差异，定义为：
$$
\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)
$$
注意，每个时刻的误差是**当前时刻的误差**，取决于下一个时刻的状态和奖励。也就是说，t时刻的误差需要到t+1时刻才能得到。

MC方法更新的目标为$G(t)$，TD方法更新的目标为$R_{t+1}+\gamma V(S_{t+1})$，乍一看好像说不通，可以结合一个例子看看

>**例子**

![alt](https://img-blog.csdnimg.cn/719f7f548ee94daea8899d540cf32fbb.png)
![alt](https://img-blog.csdnimg.cn/abf73a5e92f84d7ba925de816055b62f.png)
MC方法很好理解，我们已经经过了完整的一轮，G(t)已知，以G(t)为目标，过程会发生一些意外情况，但最后是不断逼近目标值。
TD方法属于一边经历一遍估计的学习方式，不需要经过完整一轮，不知道G(t)的值，每走一步之后，都需要根据经验对未来进行估计，最后一样可以收敛到最终值。
