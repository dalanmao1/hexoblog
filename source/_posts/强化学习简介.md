>## 机器学习的类型
- **预测型**
  * 有监督学习：根据数据预测所需输出，根据p(x)得到p(y)
  * 无监督学学习：生成数据实例，联合概率分布函数p(x|y)
- **决策型**
  * 强化学习：在动态的环境中采取行动
    + 转变到新的状态
    + 得到及时奖励
    + 随着时间的推移得到最大累积奖励
  
  **区别：** 比如智慧医疗，前者像是根据你的身体预测你可能会得某种疾病；后者是ai直接提供一个治疗方案

>## 强化学习的定义
- **智能体**
  * 感知：在某种程度上感知环境的状态
  * 行动：采取行动影响状态或者达到目标
  * 目标：获得最大的累积奖励
- **环境**
  * 智能体所处的环境，智能体的行为会改变环境
 
![Alt](pic/rldef.png#pic_center)

- **交互过程**
  * 智能体
    + 获得观测$O_t$
    + 获得奖励$R_t$
    + 做出行动$A_t$
  
  * 环境
    + 获得行动$A_t$
    + 给出观测$O_{t+1}$
    + 给出行动$R_{t+1}$

  * t在环境这一步增加
  
**例子：** 如果你在玩超级玛丽，那么游戏画面就是环境，玛丽是智能体，根据当前的游戏画面，超级玛丽会有三种行为。每一次跳跃前进后退都会改变环境（金币减少、蘑菇被吃、game over...），每一次的行为都会得到一个奖励，金币减少+1，通过+100，game over-100，因为我们的目标是获得最大累积奖励，通过不断训练不断重复，超级玛丽最终会朝着奖励最大的方向做出一系列动作。

> ## 强化学习术语
+ **历史：** 是观察、行动和奖励的序列,根据这个历史就可以知道接下来会发生什么动作。
$$
H_t=O_1,R_1,A_1,O_2,R_2,A_2,...,O_{t-1},R_{t-1},A_{t-1},Q_{t},R_t
$$

+ **状态（state）：** 处于环境中的一种状态，移动超级玛丽，状态改变
序列也可以改成：
$$
H_t=S_1,A_1,R_2,S_2,A_2,...,R_t
$$ 
+ **策略（Policy）：** 是状态到行动的映射，合格的策略能指导智能体采取最佳行动以获取最高总收益。
   *  确定性策略（Deterministic Policy）$$ a=\pi(s)$$
   *  随机策略（Stochastic Policy）$$ \pi(a|s)=P(A_t=a|S_t=s)$$
+ **奖励(Reward):**$R(s,a)$,一个标量，能立即感知什么是“好”。

+ **价值函数（Value Function）:** 用于定义长期什么是“好”，价值函数是对于未来累积奖励的**预测**。用于评估未来某个状态或者动作的好坏。
  